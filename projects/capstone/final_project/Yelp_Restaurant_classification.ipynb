{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: Yelp Restaurant Photo Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import sys\n",
    "\n",
    "import caffe\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 234842\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Training set\n",
    "############################\n",
    "\n",
    "train_photos_to_business_id = pd.read_csv(dataset_root + 'train_photo_to_biz_ids.csv')\n",
    "\n",
    "business_frequency_list = []\n",
    "count = 0\n",
    "for photo in train_photos_to_business_id['photo_id']:\n",
    "    business_frequency_list.append(train_photos_to_business_id.loc[count]['business_id'])\n",
    "    count += 1\n",
    "    \n",
    "print('Number of training images: {}'.format(len(business_frequency_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum photos in a business for training set: 2974\n",
      "Minimum photos in a business for training set: 2\n",
      "Average photos in a business for training set: 117\n"
     ]
    }
   ],
   "source": [
    "biz_frequency_dict = Counter(business_frequency_list)\n",
    "\n",
    "biz_values = biz_frequency_dict.values()\n",
    "\n",
    "biz_total_photos = sum(biz_values)\n",
    "biz_list_len = len(biz_frequency_dict.keys())\n",
    "\n",
    "biz_max_photos = max(biz_values)\n",
    "biz_min_photos = min(biz_values)\n",
    "\n",
    "print('Maximum photos in a business for training set: {}'.format(biz_max_photos))\n",
    "print('Minimum photos in a business for training set: {}'.format(biz_min_photos))\n",
    "\n",
    "print('Average photos in a business for training set: {}'.format((biz_total_photos/biz_list_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1190225\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Testing set\n",
    "############################\n",
    "\n",
    "test_photos_to_business_id = pd.read_csv(dataset_root + 'test_photo_to_biz.csv')\n",
    "\n",
    "test_biz_freq_list = []\n",
    "\n",
    "count = 0\n",
    "for photo in test_photos_to_business_id['photo_id']:\n",
    "    test_biz_freq_list.append(test_photos_to_business_id.loc[count]['business_id'])\n",
    "    count += 1\n",
    "    \n",
    "print(len(test_biz_freq_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum photos in a business for testing set: 2825\n",
      "Minimum photos in a business for testing set: 1\n",
      "Average photos in a business for testing set: 119\n"
     ]
    }
   ],
   "source": [
    "test_biz_freq_dict = Counter(test_biz_freq_list)\n",
    "\n",
    "biz_values = test_biz_freq_dict.values()\n",
    "\n",
    "biz_total_photos = sum(biz_values)\n",
    "biz_list_len = len(test_biz_freq_dict.keys())\n",
    "\n",
    "biz_max_photos = max(biz_values)\n",
    "biz_min_photos = min(biz_values)\n",
    "\n",
    "print('Maximum photos in a business for testing set: {}'.format(biz_max_photos))\n",
    "print('Minimum photos in a business for testing set: {}'.format(biz_min_photos))\n",
    "\n",
    "print('Average photos in a business for testing set: {}'.format((biz_total_photos/biz_list_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero labeled business in train set: 4\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Visualizing Training set\n",
    "############################\n",
    "\n",
    "y_training_labels = pd.read_csv(dataset_root + 'train.csv')\n",
    "\n",
    "list_of_labels = []\n",
    "count_of_non_zero_labels = 0\n",
    "\n",
    "total_count = 0\n",
    "for labels in y_training_labels['labels']:\n",
    "    if (type(labels) is str):\n",
    "        label_list = labels.split(' ')\n",
    "        list_of_labels.append(label_list)\n",
    "        count_of_non_zero_labels += 1\n",
    "    \n",
    "    total_count += 1\n",
    "\n",
    "list_of_labels = np.concatenate(list_of_labels)\n",
    "\n",
    "frequency_list = Counter(list_of_labels)\n",
    "\n",
    "print('Number of zero labeled business in train set: {}'.format((total_count - count_of_non_zero_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a39552d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting bar chart\n",
    "\n",
    "N = len(frequency_list)\n",
    "keys = list(frequency_list.keys())\n",
    "values = list(frequency_list.values())\n",
    "\n",
    "plt.bar(range(N), values, align='center')\n",
    "plt.xticks(range(N), keys)\n",
    "plt.ylabel('Number of Photos in training set')\n",
    "plt.xlabel('Label for a restaurant')\n",
    "plt.title('Frequency of photos per label in training set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of training set with labels\n",
      "\n",
      "   business_id         labels\n",
      "0         1000  1 2 3 4 5 6 7\n",
      "1         1001        0 1 6 8\n",
      "2          100    1 2 4 5 6 7\n",
      "3         1006      1 2 4 5 6\n",
      "4         1010          0 6 8\n"
     ]
    }
   ],
   "source": [
    "print('Sample of training set with labels\\n')\n",
    "print(y_training_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of training photos to training business ids\n",
      "\n",
      "   photo_id  business_id\n",
      "0    204149         3034\n",
      "1     52779         2805\n",
      "2    278973          485\n",
      "3    195284          485\n",
      "4     19992          485\n"
     ]
    }
   ],
   "source": [
    "## Load mapping of Training Photos to Business IDs\n",
    "train_photos_to_business_id = pd.read_csv(dataset_root + 'train_photo_to_biz_ids.csv')\n",
    "\n",
    "print('Mapping of training photos to training business ids\\n')\n",
    "print(train_photos_to_business_id[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of testing photos to training business ids\n",
      "\n",
      "      photo_id business_id\n",
      "1000    238721       021oz\n",
      "1001    407713       021oz\n",
      "1002    223604       021oz\n",
      "1003    408898       021oz\n",
      "1004    276062       021oz\n"
     ]
    }
   ],
   "source": [
    "## Load mapping of Testing Photos to Business IDs\n",
    "test_photos_to_business_id = pd.read_csv(dataset_root + 'test_photo_to_biz.csv')\n",
    "\n",
    "print('Mapping of testing photos to training business ids\\n')\n",
    "print(test_photos_to_business_id[1000:1005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Caffe Pre-trained Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaffeNet found.\n"
     ]
    }
   ],
   "source": [
    "# Define caffe and dataset root\n",
    "caffe_root = '/home/ubuntu/src/caffe/'\n",
    "dataset_root = '/home/ubuntu/yelp_classification/data/'\n",
    "\n",
    "test_dataset_root = '/home/ubuntu/'\n",
    "\n",
    "# Using python, so insert python into caffe root\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "# Check to see if CaffeNet is already downloaded, otherwise download it\n",
    "if os.path.isfile(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'):\n",
    "    print('CaffeNet found.')\n",
    "else:\n",
    "    print('Downloading pre-trained CaffeNet model...')\n",
    "    !/home/ubuntu/src/caffe/scripts/download_model_binary.py /home/ubuntu/src/caffe/models/bvlc_reference_caffenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use GPU    \n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Image features from the second last layer ('fc7') of CaffeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(image_list, layer = 'fc7'):\n",
    "    \n",
    "    #######################################################################################\n",
    "    # Reference: https://github.com/BVLC/caffe/blob/master/examples/00-classification.ipynb\n",
    "    #######################################################################################\n",
    "\n",
    "    # BVLC Caffenet model definition (layers etc)\n",
    "    proto_file = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\n",
    "    \n",
    "    # BVLC Caffenet learned model weights\n",
    "    caffemodel = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "\n",
    "    # Create a caffe 'Net'\n",
    "    caffe_net = caffe.Net(proto_file, caffemodel, caffe.TEST)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Pre-process the input images in form Caffe expects\n",
    "    \n",
    "    # Mean from image net\n",
    "    mean_imagenet = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "    \n",
    "    # average over pixels to obtain the mean (BGR) pixel values\n",
    "    mean = mean_imagenet.mean(1).mean(1)\n",
    "    \n",
    "    \n",
    "    # Create a transformer for loading images in form of caffenet and name it 'data'\n",
    "    \n",
    "    # By default the caffe transformer returns data shape as (10, 3, 227, 227).\n",
    "    # This is because 10 random 227x227 crops are supposed to be extracted from a 256x256 image \n",
    "    # and passed through the net.\n",
    "    transformer = caffe.io.Transformer({'data': caffe_net.blobs['data'].data.shape})\n",
    "    \n",
    "    # Transform image channels, input is (HxWxC) while caffe expects (CxHxW)\n",
    "    transformer.set_transpose('data', (2,0,1))\n",
    "\n",
    "    # subtract the dataset-mean value in each channel\n",
    "    transformer.set_mean('data', mean)\n",
    "    \n",
    "    # rescale image data from [0, 1] to [0, 255] as caffe operates on images in range [0, 255] \n",
    "    transformer.set_raw_scale('data', 255)\n",
    "    \n",
    "    # Caffe expects images in BGR format while input is in RGB format, so swap it\n",
    "    transformer.set_channel_swap('data', (2,1,0))\n",
    "\n",
    "    \n",
    "    \n",
    "    total_images = len(image_list)\n",
    "    \n",
    "    # set the size of the input (batch_size, channel, height, width)\n",
    "    caffe_net.blobs['data'].reshape(total_images, 3, 227, 227)\n",
    "    \n",
    "    # Load the images and transform them and save it in memory\n",
    "    caffe_net.blobs['data'].data[...] = map(lambda img: transformer.preprocess('data', caffe.io.load_image(img)), image_list)\n",
    "    \n",
    "    ### perform classification using BVLC reference model 'caffe_net' created earlier\n",
    "    out = caffe_net.forward()\n",
    "\n",
    "    # Return the 'fc7' layer weights features\n",
    "    return caffe_net.blobs[layer].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from training images and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Train image features file!\n"
     ]
    }
   ],
   "source": [
    "# Save the extracted features for both test and train images for later use using H5PY library\n",
    "\n",
    "# Read and close file first\n",
    "h5py_train_file = h5py.File(data_loc + 'train_images_fc7_features.h5', 'r+')\n",
    "h5py_train_file.close()\n",
    "\n",
    "\n",
    "\n",
    "# Open a h5py file for writing training features extracted using bvlc reference model\n",
    "h5py_train_file = h5py.File(data_loc + 'train_images_fc7_features.h5','w')\n",
    "\n",
    "# Create dataset for image name\n",
    "h5py_train_img_name = h5py_train_file.create_dataset('image_name', (0,), maxshape=(None,), dtype='|S54')\n",
    "\n",
    "# Create dataset for image features\n",
    "# Save features returned from bvlc fc7 layer (4096 features) to h5py file\n",
    "h5py_train_img_feature = h5py_train_file.create_dataset('image_feature', (0,4096), maxshape = (None,4096))\n",
    "\n",
    "h5py_train_file.close()\n",
    "\n",
    "print(\"Created Train image features file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 234842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the Image full path using photo list from csv file\n",
    "train_images = [os.path.join(dataset_root + 'train_photos/', str(x) + '.jpg') for x in train_photos_to_business_id['photo_id']]\n",
    "\n",
    "total_train_images = len(train_images)\n",
    "print(\"Total training images: {}\".format(total_train_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images processed: 10000\n",
      "Total images processed: 20000\n",
      "Total images processed: 30000\n",
      "Total images processed: 40000\n",
      "Total images processed: 50000\n",
      "Total images processed: 60000\n",
      "Total images processed: 70000\n",
      "Total images processed: 80000\n",
      "Total images processed: 90000\n",
      "Total images processed: 100000\n",
      "Total images processed: 110000\n",
      "Total images processed: 120000\n",
      "Total images processed: 130000\n",
      "Total images processed: 140000\n",
      "Total images processed: 150000\n",
      "Total images processed: 160000\n",
      "Total images processed: 170000\n",
      "Total images processed: 180000\n",
      "Total images processed: 190000\n",
      "Total images processed: 200000\n",
      "Total images processed: 210000\n",
      "Total images processed: 220000\n",
      "Total images processed: 230000\n",
      "Total images processed: 234842\n"
     ]
    }
   ],
   "source": [
    "# Batch size to process image features\n",
    "batch_size = 500\n",
    "\n",
    "# Get the features for training Images\n",
    "for count in range(0, total_train_images, batch_size):\n",
    "    \n",
    "    # Get the image path for this batch of images\n",
    "    image_path = train_images[count: min((count + batch_size), total_train_images)]\n",
    "    \n",
    "    # Get features for all batch images\n",
    "    train_img_features = load_features(image_path, layer='fc7')\n",
    "    \n",
    "    total_processed = count + train_img_features.shape[0]\n",
    "\n",
    "    \n",
    "    # Open the h5 file to store the image features for future use\n",
    "    h5py_train_file = h5py.File(data_loc + 'train_images_fc7_features.h5','r+')\n",
    "    \n",
    "    h5py_train_file['image_name'].resize((total_processed,))\n",
    "    h5py_train_file['image_name'][count : total_processed] = np.array(image_path)\n",
    "    \n",
    "    h5py_train_file['image_feature'].resize((total_processed, train_img_features.shape[1]))\n",
    "    h5py_train_file['image_feature'][count : total_processed, :] = train_img_features\n",
    "    h5py_train_file.close()\n",
    "\n",
    "    if (total_processed % 10000) == 0 or total_processed == total_train_images:\n",
    "        print(\"Total images processed: {}\".format(total_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from Testing images and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Test image features file!\n"
     ]
    }
   ],
   "source": [
    "# Save the extracted features for both test and train images for later use using H5PY library\n",
    "\n",
    "# Read and close file first\n",
    "h5py_test_file = h5py.File(dataset_root + 'test_images_fc7_features.h5', 'r+')\n",
    "h5py_test_file.close()\n",
    "\n",
    "\n",
    "\n",
    "# Open a h5py file for writing testing features extracted using bvlc reference model\n",
    "h5py_test_file = h5py.File(dataset_root + 'test_images_fc7_features.h5','w')\n",
    "\n",
    "# Create dataset for image name\n",
    "h5py_test_img_name = h5py_test_file.create_dataset('image_name', (0,), maxshape=(None,), dtype='|S54')\n",
    "\n",
    "# Create dataset for image features\n",
    "# Save features returned from bvlc fc7 layer (4096 features) to h5py file\n",
    "h5py_test_img_feature = h5py_test_file.create_dataset('image_feature', (0,4096), maxshape = (None,4096))\n",
    "\n",
    "h5py_test_file.close()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Created Test image features file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test photos to business id: 1190225\n",
      "Total unique test photos: 237152\n",
      "Total test images: 237152\n"
     ]
    }
   ],
   "source": [
    "print(\"Total test photos to business id: {}\".format(len(test_photos_to_business_id)))\n",
    "\n",
    "print(\"Total unique test photos: {}\".format(len(test_photos_to_business_id['photo_id'].unique())))\n",
    "\n",
    "\n",
    "# Get the Image full path using photo list from csv file\n",
    "test_images = [os.path.join(test_dataset_root + 'test_photos/', str(x) + '.jpg') for x in test_photos_to_business_id['photo_id'].unique()]\n",
    "\n",
    "total_test_images = len(test_images)\n",
    "print(\"Total test images: {}\".format(total_test_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test images processed: 1000\n",
      "Total test images processed: 2000\n",
      "Total test images processed: 3000\n",
      "Total test images processed: 4000\n",
      "Total test images processed: 5000\n",
      "Total test images processed: 6000\n",
      "Total test images processed: 7000\n",
      "Total test images processed: 8000\n",
      "Total test images processed: 9000\n",
      "Total test images processed: 10000\n",
      "Total test images processed: 11000\n",
      "Total test images processed: 12000\n",
      "Total test images processed: 13000\n",
      "Total test images processed: 14000\n",
      "Total test images processed: 15000\n",
      "Total test images processed: 16000\n",
      "Total test images processed: 17000\n",
      "Total test images processed: 18000\n",
      "Total test images processed: 19000\n",
      "Total test images processed: 20000\n",
      "Total test images processed: 21000\n",
      "Total test images processed: 22000\n",
      "Total test images processed: 23000\n",
      "Total test images processed: 24000\n",
      "Total test images processed: 25000\n",
      "Total test images processed: 26000\n",
      "Total test images processed: 27000\n",
      "Total test images processed: 28000\n",
      "Total test images processed: 29000\n",
      "Total test images processed: 30000\n",
      "Total test images processed: 31000\n",
      "Total test images processed: 32000\n",
      "Total test images processed: 33000\n",
      "Total test images processed: 34000\n",
      "Total test images processed: 35000\n",
      "Total test images processed: 36000\n",
      "Total test images processed: 37000\n",
      "Total test images processed: 38000\n",
      "Total test images processed: 39000\n",
      "Total test images processed: 40000\n",
      "Total test images processed: 41000\n",
      "Total test images processed: 42000\n",
      "Total test images processed: 43000\n",
      "Total test images processed: 44000\n",
      "Total test images processed: 45000\n",
      "Total test images processed: 46000\n",
      "Total test images processed: 47000\n",
      "Total test images processed: 48000\n",
      "Total test images processed: 49000\n",
      "Total test images processed: 50000\n",
      "Total test images processed: 51000\n",
      "Total test images processed: 52000\n",
      "Total test images processed: 53000\n",
      "Total test images processed: 54000\n",
      "Total test images processed: 55000\n",
      "Total test images processed: 56000\n",
      "Total test images processed: 57000\n",
      "Total test images processed: 58000\n",
      "Total test images processed: 59000\n",
      "Total test images processed: 60000\n",
      "Total test images processed: 61000\n",
      "Total test images processed: 62000\n",
      "Total test images processed: 63000\n",
      "Total test images processed: 64000\n",
      "Total test images processed: 65000\n",
      "Total test images processed: 66000\n",
      "Total test images processed: 67000\n",
      "Total test images processed: 68000\n",
      "Total test images processed: 69000\n",
      "Total test images processed: 70000\n",
      "Total test images processed: 71000\n",
      "Total test images processed: 72000\n",
      "Total test images processed: 73000\n",
      "Total test images processed: 74000\n",
      "Total test images processed: 75000\n",
      "Total test images processed: 76000\n",
      "Total test images processed: 77000\n",
      "Total test images processed: 78000\n",
      "Total test images processed: 79000\n",
      "Total test images processed: 80000\n",
      "Total test images processed: 81000\n",
      "Total test images processed: 82000\n",
      "Total test images processed: 83000\n",
      "Total test images processed: 84000\n",
      "Total test images processed: 85000\n",
      "Total test images processed: 86000\n",
      "Total test images processed: 87000\n",
      "Total test images processed: 88000\n",
      "Total test images processed: 89000\n",
      "Total test images processed: 90000\n",
      "Total test images processed: 91000\n",
      "Total test images processed: 92000\n",
      "Total test images processed: 93000\n",
      "Total test images processed: 94000\n",
      "Total test images processed: 95000\n",
      "Total test images processed: 96000\n",
      "Total test images processed: 97000\n",
      "Total test images processed: 98000\n",
      "Total test images processed: 99000\n",
      "Total test images processed: 100000\n",
      "Total test images processed: 101000\n",
      "Total test images processed: 102000\n",
      "Total test images processed: 103000\n",
      "Total test images processed: 104000\n",
      "Total test images processed: 105000\n",
      "Total test images processed: 106000\n",
      "Total test images processed: 107000\n",
      "Total test images processed: 108000\n",
      "Total test images processed: 109000\n",
      "Total test images processed: 110000\n",
      "Total test images processed: 111000\n",
      "Total test images processed: 112000\n",
      "Total test images processed: 113000\n",
      "Total test images processed: 114000\n",
      "Total test images processed: 115000\n",
      "Total test images processed: 116000\n",
      "Total test images processed: 117000\n",
      "Total test images processed: 118000\n",
      "Total test images processed: 119000\n",
      "Total test images processed: 120000\n",
      "Total test images processed: 121000\n",
      "Total test images processed: 122000\n",
      "Total test images processed: 123000\n",
      "Total test images processed: 124000\n",
      "Total test images processed: 125000\n",
      "Total test images processed: 126000\n",
      "Total test images processed: 127000\n",
      "Total test images processed: 128000\n",
      "Total test images processed: 129000\n",
      "Total test images processed: 130000\n",
      "Total test images processed: 131000\n",
      "Total test images processed: 132000\n",
      "Total test images processed: 133000\n",
      "Total test images processed: 134000\n",
      "Total test images processed: 135000\n",
      "Total test images processed: 136000\n",
      "Total test images processed: 137000\n",
      "Total test images processed: 138000\n",
      "Total test images processed: 139000\n",
      "Total test images processed: 140000\n",
      "Total test images processed: 141000\n",
      "Total test images processed: 142000\n",
      "Total test images processed: 143000\n",
      "Total test images processed: 144000\n",
      "Total test images processed: 145000\n",
      "Total test images processed: 146000\n",
      "Total test images processed: 147000\n",
      "Total test images processed: 148000\n",
      "Total test images processed: 149000\n",
      "Total test images processed: 150000\n",
      "Total test images processed: 151000\n",
      "Total test images processed: 152000\n",
      "Total test images processed: 153000\n",
      "Total test images processed: 154000\n",
      "Total test images processed: 155000\n",
      "Total test images processed: 156000\n",
      "Total test images processed: 157000\n",
      "Total test images processed: 158000\n",
      "Total test images processed: 159000\n",
      "Total test images processed: 160000\n",
      "Total test images processed: 161000\n",
      "Total test images processed: 162000\n",
      "Total test images processed: 163000\n",
      "Total test images processed: 164000\n",
      "Total test images processed: 165000\n",
      "Total test images processed: 166000\n",
      "Total test images processed: 167000\n",
      "Total test images processed: 168000\n",
      "Total test images processed: 169000\n",
      "Total test images processed: 170000\n",
      "Total test images processed: 171000\n",
      "Total test images processed: 172000\n",
      "Total test images processed: 173000\n",
      "Total test images processed: 174000\n",
      "Total test images processed: 175000\n",
      "Total test images processed: 176000\n",
      "Total test images processed: 177000\n",
      "Total test images processed: 178000\n",
      "Total test images processed: 179000\n",
      "Total test images processed: 180000\n",
      "Total test images processed: 181000\n",
      "Total test images processed: 182000\n",
      "Total test images processed: 183000\n",
      "Total test images processed: 184000\n",
      "Total test images processed: 185000\n",
      "Total test images processed: 186000\n",
      "Total test images processed: 187000\n",
      "Total test images processed: 188000\n",
      "Total test images processed: 189000\n",
      "Total test images processed: 190000\n",
      "Total test images processed: 191000\n",
      "Total test images processed: 192000\n",
      "Total test images processed: 193000\n",
      "Total test images processed: 194000\n",
      "Total test images processed: 195000\n",
      "Total test images processed: 196000\n",
      "Total test images processed: 197000\n",
      "Total test images processed: 198000\n",
      "Total test images processed: 199000\n",
      "Total test images processed: 200000\n",
      "Total test images processed: 201000\n",
      "Total test images processed: 202000\n",
      "Total test images processed: 203000\n",
      "Total test images processed: 204000\n",
      "Total test images processed: 205000\n",
      "Total test images processed: 206000\n",
      "Total test images processed: 207000\n",
      "Total test images processed: 208000\n",
      "Total test images processed: 209000\n",
      "Total test images processed: 210000\n",
      "Total test images processed: 211000\n",
      "Total test images processed: 212000\n",
      "Total test images processed: 213000\n",
      "Total test images processed: 214000\n",
      "Total test images processed: 215000\n",
      "Total test images processed: 216000\n",
      "Total test images processed: 217000\n",
      "Total test images processed: 218000\n",
      "Total test images processed: 219000\n",
      "Total test images processed: 220000\n",
      "Total test images processed: 221000\n",
      "Total test images processed: 222000\n",
      "Total test images processed: 223000\n",
      "Total test images processed: 224000\n",
      "Total test images processed: 225000\n",
      "Total test images processed: 226000\n",
      "Total test images processed: 227000\n",
      "Total test images processed: 228000\n",
      "Total test images processed: 229000\n",
      "Total test images processed: 230000\n",
      "Total test images processed: 231000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test images processed: 232000\n",
      "Total test images processed: 233000\n",
      "Total test images processed: 234000\n",
      "Total test images processed: 235000\n",
      "Total test images processed: 236000\n",
      "Total test images processed: 237000\n",
      "Total test images processed: 237152\n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch size to process image features\n",
    "batch_size = 500\n",
    "\n",
    "# Get the features for testing Images\n",
    "for count in range(0, total_test_images, batch_size):\n",
    "    \n",
    "    # Get the image path for this batch of images\n",
    "    image_path = test_images[count: min((count + batch_size), total_test_images)]\n",
    "    \n",
    "    # Get features for all batch images\n",
    "    test_img_features = load_features(image_path, layer='fc7')\n",
    "    \n",
    "    total_processed = (count + test_img_features.shape[0])\n",
    "\n",
    "    \n",
    "    # Open the h5 file to store the image features for future use\n",
    "    h5py_test_file = h5py.File(dataset_root + 'test_images_fc7_features.h5','r+')\n",
    "    \n",
    "    h5py_test_file['image_name'].resize((total_processed,))\n",
    "    h5py_test_file['image_name'][count : total_processed] = np.array(image_path)\n",
    "    \n",
    "    h5py_test_file['image_feature'].resize((total_processed, test_img_features.shape[1]))\n",
    "    h5py_test_file['image_feature'][count : total_processed, :] = test_img_features\n",
    "    h5py_test_file.close()\n",
    "\n",
    "    if (total_processed % 1000) == 0 or total_processed == total_test_images:\n",
    "        print(\"Total test images processed: {}\".format(total_processed))\n",
    "        \n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate train image features to each business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dropping business with no labels\n",
    "y_training_labels = pd.read_csv(dataset_root + 'train.csv').dropna()\n",
    "#print(training_labels[:5])\n",
    "\n",
    "\n",
    "# Convert space delimitted labels into tuples\n",
    "y_training_labels['labels'] = y_training_labels['labels'].apply(lambda labels: tuple(sorted(int(label) for label in labels.split())))\n",
    "#print(training_labels[:5])\n",
    "\n",
    "\n",
    "# Set the main index of labels to business_id\n",
    "y_training_labels.set_index('business_id', inplace=True)\n",
    "#print(\"Training data after setting index to business id\")\n",
    "#print(training_labels[:5])\n",
    "\n",
    "\n",
    "# Get the unique businesses in train file\n",
    "train_business_ids = y_training_labels.index.unique()\n",
    "#print(\"Business id\")\n",
    "#print(business_ids[:5])\n",
    "#print('Total businesses: {}'.format(len(business_ids)))\n",
    "\n",
    "\n",
    "\n",
    "# Load the training features from h5 file\n",
    "X_train_features_file = h5py.File(dataset_root + 'train_images_fc7_features.h5', 'r')\n",
    "X_train_features = np.copy(X_train_features_file['image_feature'])\n",
    "X_train_features_file.close()\n",
    "\n",
    "\n",
    "\n",
    "# Create a new DataFrame to store business, labels along with image features now\n",
    "train_data_frame = pd.DataFrame(columns=['business_id', 'label', 'features'])\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done pass: 1996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "\n",
    "for business in train_business_ids:\n",
    "    #if count < 1:\n",
    "    #    print('Business: {}'.format(business))\n",
    "    \n",
    "    \n",
    "    label_from_business = y_training_labels.loc[business]['labels']\n",
    "    #if count < 1:\n",
    "    #    print('Labels from business: {}'.format(label_from_business))\n",
    "    \n",
    "    \n",
    "    business_list = train_photos_to_business_id[train_photos_to_business_id['business_id'] == business]\n",
    "    #if count < 1:\n",
    "    #    print('business_list:\\n')\n",
    "    #    print(business_list)\n",
    "    \n",
    "    \n",
    "    business_list_index = train_photos_to_business_id[train_photos_to_business_id['business_id'] == business].index\n",
    "    #if count < 1:\n",
    "    #    print('business_list_index:\\n')\n",
    "    #    print(business_list_index)\n",
    "    \n",
    "    \n",
    "    image_list = train_photos_to_business_id[train_photos_to_business_id['business_id'] == business].index.tolist()\n",
    "    #if count < 1:\n",
    "    #    print('Image_list:\\n')\n",
    "    #    print(image_list)\n",
    "    \n",
    "    \n",
    "    feature_list = X_train_features[image_list]\n",
    "    \n",
    "    #if count < 1:\n",
    "    #    print('feature_list:\\n')\n",
    "    #    print(feature_list)\n",
    "     \n",
    "    \n",
    "    mean_feature = list(np.mean(feature_list, axis=0))\n",
    "    \n",
    "    #if count < 1:\n",
    "    #    print('mean_feature:\\n')\n",
    "    #    print(mean_feature)\n",
    "        \n",
    "    train_data_frame.loc[count] = [business, label_from_business, mean_feature]\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if (count == len(train_business_ids)):\n",
    "        print('Done pass: {}'.format(count))\n",
    "\n",
    "        \n",
    "# Store train data frame to file\n",
    "with open(dataset_root + \"train_business_label_fc7_features.csv\", 'w') as f:  \n",
    "    train_data_frame.to_csv(f, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five features\n",
      "   business_id                  label  \\\n",
      "0         1000  (1, 2, 3, 4, 5, 6, 7)   \n",
      "1         1001           (0, 1, 6, 8)   \n",
      "2          100     (1, 2, 4, 5, 6, 7)   \n",
      "3         1006        (1, 2, 4, 5, 6)   \n",
      "4         1010              (0, 6, 8)   \n",
      "\n",
      "                                            features  \n",
      "0  [0.19977085, 0.43287092, 0.22732987, 0.3551694...  \n",
      "1  [0.0, 0.58893245, 0.53906047, 0.17221628, 0.01...  \n",
      "2  [0.11155061, 0.034822084, 0.12025276, 0.520122...  \n",
      "3  [0.078059338, 0.054452635, 0.05638162, 0.69423...  \n",
      "4  [0.39657032, 0.27962369, 0.0, 0.17205141, 0.36...  \n",
      "Total length of features\n",
      "1996\n"
     ]
    }
   ],
   "source": [
    "# Test if the features saved correctly\n",
    "train_feature_csv = pd.read_csv(dataset_root + 'train_business_label_fc7_features.csv')\n",
    "\n",
    "print('First five features')\n",
    "print(train_feature_csv[:5])\n",
    "print('Total length of features')\n",
    "print(len(train_feature_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate testing image features to each business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "## Load mapping of testing Photos to Business IDs\n",
    "test_photo_to_business_id = pd.read_csv(dataset_root + 'test_photo_to_biz.csv')\n",
    "#print(test_photo_to_business_id[:5])\n",
    "\n",
    "test_business_ids = test_photo_to_business_id['business_id'].unique()\n",
    "\n",
    "\n",
    "\n",
    "# Load the testing features from h5 file\n",
    "X_test_features_file = h5py.File(dataset_root + 'test_images_fc7_features.h5', 'r')\n",
    "X_test_image_name = list(np.copy(X_test_features_file['image_name']))\n",
    "X_test_image_features = np.copy(X_test_features_file['image_feature'])\n",
    "X_test_features_file.close()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for train_list_imgs in X_test_image_name:\n",
    "    #print(train_list_imgs)\n",
    "    if \"317818\" in train_list_imgs:\n",
    "        #print(train_list_imgs)\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/test_photos/317818.jpg', '/home/ubuntu/test_photos/30679.jpg', '/home/ubuntu/test_photos/455084.jpg', '/home/ubuntu/test_photos/371381.jpg', '/home/ubuntu/test_photos/86224.jpg']\n",
      "['317818.jpg', '30679.jpg', '455084.jpg', '371381.jpg', '86224.jpg']\n",
      "['317818', '30679', '455084', '371381', '86224']\n"
     ]
    }
   ],
   "source": [
    "print(X_test_image_name[:5])\n",
    "\n",
    "X_test_image_name_short = [name.split('/')[-1] for name in X_test_image_name]\n",
    "\n",
    "print(X_test_image_name_short[:5])\n",
    "\n",
    "X_test_image_name_short_without_ext = [name.split('.')[0] for name in X_test_image_name_short]\n",
    "\n",
    "print(X_test_image_name_short_without_ext[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buisness processed:  100 Time passed:  11.9 sec\n",
      "Buisness processed:  200 Time passed:  26.6 sec\n",
      "Buisness processed:  300 Time passed:  44.1 sec\n",
      "Buisness processed:  400 Time passed:  60.6 sec\n",
      "Buisness processed:  500 Time passed:  79.8 sec\n",
      "Buisness processed:  600 Time passed:  103.4 sec\n",
      "Buisness processed:  700 Time passed:  127.2 sec\n",
      "Buisness processed:  800 Time passed:  153.0 sec\n",
      "Buisness processed:  900 Time passed:  177.6 sec\n",
      "Buisness processed:  1000 Time passed:  199.4 sec\n",
      "Buisness processed:  1100 Time passed:  228.3 sec\n",
      "Buisness processed:  1200 Time passed:  256.1 sec\n",
      "Buisness processed:  1300 Time passed:  285.2 sec\n",
      "Buisness processed:  1400 Time passed:  317.3 sec\n",
      "Buisness processed:  1500 Time passed:  348.1 sec\n",
      "Buisness processed:  1600 Time passed:  378.3 sec\n",
      "Buisness processed:  1700 Time passed:  409.8 sec\n",
      "Buisness processed:  1800 Time passed:  441.1 sec\n",
      "Buisness processed:  1900 Time passed:  474.1 sec\n",
      "Buisness processed:  2000 Time passed:  503.7 sec\n",
      "Buisness processed:  2100 Time passed:  531.7 sec\n",
      "Buisness processed:  2200 Time passed:  567.3 sec\n",
      "Buisness processed:  2300 Time passed:  603.1 sec\n",
      "Buisness processed:  2400 Time passed:  634.3 sec\n",
      "Buisness processed:  2500 Time passed:  667.4 sec\n",
      "Buisness processed:  2600 Time passed:  699.9 sec\n",
      "Buisness processed:  2700 Time passed:  729.3 sec\n",
      "Buisness processed:  2800 Time passed:  754.9 sec\n",
      "Buisness processed:  2900 Time passed:  785.2 sec\n",
      "Buisness processed:  3000 Time passed:  820.7 sec\n",
      "Buisness processed:  3100 Time passed:  863.5 sec\n",
      "Buisness processed:  3200 Time passed:  907.0 sec\n",
      "Buisness processed:  3300 Time passed:  940.6 sec\n",
      "Buisness processed:  3400 Time passed:  971.2 sec\n",
      "Buisness processed:  3500 Time passed:  1000.3 sec\n",
      "Buisness processed:  3600 Time passed:  1046.9 sec\n",
      "Buisness processed:  3700 Time passed:  1082.4 sec\n",
      "Buisness processed:  3800 Time passed:  1115.3 sec\n",
      "Buisness processed:  3900 Time passed:  1156.2 sec\n",
      "Buisness processed:  4000 Time passed:  1187.1 sec\n",
      "Buisness processed:  4100 Time passed:  1218.1 sec\n",
      "Buisness processed:  4200 Time passed:  1250.6 sec\n",
      "Buisness processed:  4300 Time passed:  1284.2 sec\n",
      "Buisness processed:  4400 Time passed:  1328.5 sec\n",
      "Buisness processed:  4500 Time passed:  1367.2 sec\n",
      "Buisness processed:  4600 Time passed:  1405.0 sec\n",
      "Buisness processed:  4700 Time passed:  1441.6 sec\n",
      "Buisness processed:  4800 Time passed:  1477.7 sec\n",
      "Buisness processed:  4900 Time passed:  1510.7 sec\n",
      "Buisness processed:  5000 Time passed:  1540.7 sec\n",
      "Buisness processed:  5100 Time passed:  1573.5 sec\n",
      "Buisness processed:  5200 Time passed:  1609.7 sec\n",
      "Buisness processed:  5300 Time passed:  1641.8 sec\n",
      "Buisness processed:  5400 Time passed:  1677.4 sec\n",
      "Buisness processed:  5500 Time passed:  1708.9 sec\n",
      "Buisness processed:  5600 Time passed:  1739.0 sec\n",
      "Buisness processed:  5700 Time passed:  1776.1 sec\n",
      "Buisness processed:  5800 Time passed:  1816.9 sec\n",
      "Buisness processed:  5900 Time passed:  1846.1 sec\n",
      "Buisness processed:  6000 Time passed:  1877.3 sec\n",
      "Buisness processed:  6100 Time passed:  1911.6 sec\n",
      "Buisness processed:  6200 Time passed:  1950.6 sec\n",
      "Buisness processed:  6300 Time passed:  1980.0 sec\n",
      "Buisness processed:  6400 Time passed:  2016.0 sec\n",
      "Buisness processed:  6500 Time passed:  2050.6 sec\n",
      "Buisness processed:  6600 Time passed:  2078.6 sec\n",
      "Buisness processed:  6700 Time passed:  2106.1 sec\n",
      "Buisness processed:  6800 Time passed:  2144.7 sec\n",
      "Buisness processed:  6900 Time passed:  2183.4 sec\n",
      "Buisness processed:  7000 Time passed:  2222.3 sec\n",
      "Buisness processed:  7100 Time passed:  2257.0 sec\n",
      "Buisness processed:  7200 Time passed:  2290.3 sec\n",
      "Buisness processed:  7300 Time passed:  2322.9 sec\n",
      "Buisness processed:  7400 Time passed:  2357.3 sec\n",
      "Buisness processed:  7500 Time passed:  2390.4 sec\n",
      "Buisness processed:  7600 Time passed:  2420.7 sec\n",
      "Buisness processed:  7700 Time passed:  2460.7 sec\n",
      "Buisness processed:  7800 Time passed:  2498.4 sec\n",
      "Buisness processed:  7900 Time passed:  2535.6 sec\n",
      "Buisness processed:  8000 Time passed:  2566.3 sec\n",
      "Buisness processed:  8100 Time passed:  2598.4 sec\n",
      "Buisness processed:  8200 Time passed:  2630.6 sec\n",
      "Buisness processed:  8300 Time passed:  2659.1 sec\n",
      "Buisness processed:  8400 Time passed:  2697.4 sec\n",
      "Buisness processed:  8500 Time passed:  2733.6 sec\n",
      "Buisness processed:  8600 Time passed:  2768.8 sec\n",
      "Buisness processed:  8700 Time passed:  2810.7 sec\n",
      "Buisness processed:  8800 Time passed:  2844.1 sec\n",
      "Buisness processed:  8900 Time passed:  2882.8 sec\n",
      "Buisness processed:  9000 Time passed:  2915.9 sec\n",
      "Buisness processed:  9100 Time passed:  2945.4 sec\n",
      "Buisness processed:  9200 Time passed:  2974.3 sec\n",
      "Buisness processed:  9300 Time passed:  3017.9 sec\n",
      "Buisness processed:  9400 Time passed:  3048.9 sec\n",
      "Buisness processed:  9500 Time passed:  3082.3 sec\n",
      "Buisness processed:  9600 Time passed:  3123.5 sec\n",
      "Buisness processed:  9700 Time passed:  3168.2 sec\n",
      "Buisness processed:  9800 Time passed:  3204.2 sec\n",
      "Buisness processed:  9900 Time passed:  3246.8 sec\n",
      "Buisness processed:  10000 Time passed:  3283.0 sec\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "test_data_frame = pd.DataFrame(columns=['business_id', 'features'])\n",
    "count = 0\n",
    "t = time.time()\n",
    "\n",
    "for business in test_business_ids:     \n",
    "    \n",
    "    #print('Processing business ID: {}'.format(business))\n",
    "    \n",
    "    photo_ids = test_photo_to_business_id[test_photo_to_business_id['business_id'] == business]['photo_id'].tolist()  \n",
    "    \n",
    "    #print('Photo IDs: {}'.format(photo_ids))\n",
    "    \n",
    "    #for ph in photo_ids:\n",
    "    #    print('Photo: {}'.format(ph))\n",
    "    #    print('X test: {}'.format(X_test_image_name_short_without_ext.index(str(ph))))\n",
    "    \n",
    "    image_index = [X_test_image_name_short_without_ext.index(str(photo)) for photo in photo_ids]\n",
    "               \n",
    "    test_features = X_test_image_features[image_index]\n",
    "    \n",
    "    test_mean_feature = list(np.mean(test_features, axis=0))\n",
    "\n",
    "    test_data_frame.loc[count] = [business, test_mean_feature]\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if (count % 100) == 0:\n",
    "        print \"Buisness processed: \", count, \"Time passed: \", \"{0:.1f}\".format(time.time()-t), \"sec\"\n",
    "\n",
    "\n",
    "with open(dataset_root+\"test_business_fc7_features.csv\",'w') as f:  \n",
    "    test_data_frame.to_csv(f, index=False)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003sg</td>\n",
       "      <td>[0.19304767, 0.25836322, 0.19439411, 0.4623304...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00er5</td>\n",
       "      <td>[0.19397034, 0.25547439, 0.18416163, 0.3357919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00kad</td>\n",
       "      <td>[0.12130528, 0.12655617, 0.076521836, 0.383440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00mc6</td>\n",
       "      <td>[0.28427792, 0.11110595, 0.47849005, 0.4494445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00q7x</td>\n",
       "      <td>[0.23811768, 0.33041945, 0.25544992, 0.3258045...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  business_id                                           features\n",
       "0       003sg  [0.19304767, 0.25836322, 0.19439411, 0.4623304...\n",
       "1       00er5  [0.19397034, 0.25547439, 0.18416163, 0.3357919...\n",
       "2       00kad  [0.12130528, 0.12655617, 0.076521836, 0.383440...\n",
       "3       00mc6  [0.28427792, 0.11110595, 0.47849005, 0.4494445...\n",
       "4       00q7x  [0.23811768, 0.33041945, 0.25544992, 0.3258045..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check file content\n",
    "test_feature_csv = pd.read_csv(dataset_root+'test_business_fc7_features.csv')\n",
    "print test_feature_csv.shape\n",
    "test_feature_csv[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use classifier to train on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values = train_feature_csv['features'].values\n",
    "y_train_values = train_feature_csv['label'].values\n",
    "\n",
    "X_test_values = test_feature_csv['features'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_array(str_label):\n",
    "    str_label = str_label[1:-1]\n",
    "    str_label = str_label.split(',')\n",
    "    return [int(x) for x in str_label if len(x)>0]\n",
    "\n",
    "\n",
    "def convert_feature_to_vector(str_feature):\n",
    "    str_feature = str_feature[1:-1]\n",
    "    str_feature = str_feature.split(',')\n",
    "    return [float(x) for x in str_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values = np.array([convert_feature_to_vector(y) for y in train_feature_csv['features']])\n",
    "#print(X_train_values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_values = np.array([convert_label_to_array(y) for y in train_feature_csv['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_values = np.array([convert_feature_to_vector(y) for y in test_feature_csv['features']])\n",
    "#print(X_test_values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed:  302.0 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t=time.time()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "#Convert list of labels to binary matrix\n",
    "y_train_one_hot_encoded = mlb.fit_transform(y_train_values)\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_values, y_train_one_hot_encoded, test_size=.25, random_state=random_state)\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_predict = classifier.predict(X_test)\n",
    "\n",
    "print \"Time passed: \", \"{0:.1f}\".format(time.time()-t), \"sec\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)]\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Classifier details\n",
    "####################\n",
    "\n",
    "print(classifier.estimators_)\n",
    "\n",
    "print(classifier.classes_)\n",
    "\n",
    "print(classifier.multilabel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using linear SVM :  0.801305970149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print \"F1 score using linear SVM : \", f1_score(y_test, y_predict, average='micro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed for RBF SVC:  346.2 sec\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "#Convert list of labels to binary matrix\n",
    "y_train_one_hot_encoded = mlb.fit_transform(y_train_values)\n",
    "\n",
    "random_state = np.random.RandomState(10)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_values, y_train_one_hot_encoded, test_size=.25, random_state=random_state)\n",
    "\n",
    "rbf_classifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True, random_state=random_state))\n",
    "\n",
    "rbf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_predict = rbf_classifier.predict(X_test)\n",
    "\n",
    "print \"Time passed for RBF SVC: \", \"{0:.1f}\".format(time.time()-t), \"sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score using rbf kernel SVM :  0.829973915106\n"
     ]
    }
   ],
   "source": [
    "print \"F1 score using rbf kernel SVM : \", f1_score(y_test, y_predict, average='micro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Linear SVM\n",
    "########################\n",
    "\n",
    "y_predict_linear_svm = classifier.predict(X_test_values)\n",
    "\n",
    "#Convert binary matrix back to labels\n",
    "y_predict_label_linear_svm = mlb.inverse_transform(y_predict_linear_svm)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "final_test_df = pd.DataFrame(columns=['business_id', 'labels'])\n",
    "\n",
    "for i in range(len(test_feature_csv)):\n",
    "    \n",
    "    biz = test_feature_csv.loc[i]['business_id']\n",
    "    label = y_predict_label_linear_svm[i]\n",
    "    label = str(label)[1:-1].replace(\",\", \" \")\n",
    "    final_test_df.loc[i] = [str(biz), label]\n",
    "\n",
    "with open(dataset_root+\"yelp_test_data_submission_linear_svm.csv\",'w') as f:\n",
    "    final_test_df.to_csv(f, index=False)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# RBF SVM\n",
    "########################\n",
    "\n",
    "y_predict_rbf_svm = rbf_classifier.predict(X_test_values)\n",
    "\n",
    "#Convert binary matrix back to labels\n",
    "y_predict_label_rbf_svm = mlb.inverse_transform(y_predict_rbf_svm)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "final_test_df = pd.DataFrame(columns=['business_id', 'labels'])\n",
    "\n",
    "for i in range(len(test_feature_csv)):\n",
    "    \n",
    "    biz = test_feature_csv.loc[i]['business_id']\n",
    "    label = y_predict_label_rbf_svm[i]\n",
    "    label = str(label)[1:-1].replace(\",\", \" \")\n",
    "    final_test_df.loc[i] = [str(biz), label]\n",
    "\n",
    "with open(dataset_root+\"yelp_test_data_submission_rbf_svm.csv\",'w') as f:\n",
    "    final_test_df.to_csv(f, index=False)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 1]\n",
      " [0 0 0 1 0 1 1 0 1]\n",
      " [0 1 0 0 1 0 1 0 0]\n",
      " [0 1 0 0 1 1 1 0 1]]\n",
      "[(1, 2, 3, 5, 6), (1, 2, 5, 6, 8), (3, 5, 6, 8), (1, 4, 6), (1, 4, 5, 6, 8)]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict_linear_svm[:5])\n",
    "print(y_predict_label_linear_svm[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 1 0 0]\n",
      " [0 1 1 1 0 1 1 0 1]\n",
      " [0 1 1 0 0 1 1 0 1]\n",
      " [0 1 1 1 0 1 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 0]]\n",
      "[(1, 2, 3, 5, 6), (1, 2, 3, 5, 6, 8), (1, 2, 5, 6, 8), (1, 2, 3, 5, 6), (1, 2, 5, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict_rbf_svm[:5])\n",
    "print(y_predict_label_rbf_svm[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
